//
// Copyright 2022 Nuna inc.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//      https://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//

import examples.claim
import examples.dataproc
import dataset
import dataset_spark
import flags

GENERATORS_RE : Tuple = [
  Tuple([".*npi_id", Tuple(["str", Tuple([3, "ABC"])])]),
  Tuple([".*_id", Tuple(["str", Tuple([6, "0123456789ABCDEF"])])]),
  Tuple([".*_id_syn", Tuple(["str", Tuple([4, "01234"])])]),
]

def assignMember(
  member_id: String,
  claims: Array<examples.claim.Claim>,
  minimum_visit_count: UInt = 1u)=> {
  claims
    .filter((c, member_id = member_id) => c.person_id_syn == member_id )
    // Aggregate to a tuple keyd by npi_id, sets of unique visits:
    .aggregate_to_set(c => c.npi_id, c => c.visit_id)
    // Filter these tuples and keep the ones with at least minimum_visit_count
    .filter((t, min_count = minimum_visit_count) => len(t[1]) >= min_count )
    // Now aggregate by the count of visits:
    .aggregate_to_array(t => len(t[1]), t => t[0])
    // Take the one with the maximum count:
    .max_by(t => t[0])
    // And pick a random npi from the top list:
    .ensured_run(t => t[1].shuffle().front());
}

// Just generates some random test data in a csv file under gendir
// Returns the generated file name.
def generate_csv(gendir: String, count: Int = 20000) => {
  examples.dataproc.generate_csv(examples.claim.Claim(), gendir, count,
    GENERATORS_RE)
}
def generate_parquet(gendir: String, count: Int = 20000) => {
  examples.dataproc.generate_parquet(examples.claim.Claim(), gendir, count,
    GENERATORS_RE)
}

// Run analyzer on all Claims in provided CSV filename:
def analyze_array(filename: String) => {
  print("Analyzing: " + filename)
  // Read the CSV with provided schema:
  examples.dataproc.read_csv(examples.claim.Claim(), filename)
    // groups the claims by member id:
    .aggregate_to_array(c => c.person_id_syn, c => c)
    // performs assignMember for each member claims, and prints the result:
    .map(t => {
      assigned = assignMember(t[0], t[1], 2u)
      if (is_null(assigned)) {
        pass
      }
      print("Person: " + t[0] + " ===> NPI: " + _ensured(assigned));
      yield _ensured(assigned)
    })
}

def analyze_dataset(engine: dataset.Engine, filename: String) => {
  print("Analyzing w/ datasets: " + filename)
  dataset.read_parquet(engine, examples.claim.Claim(), filename)
    .aggregate(c => dataset.agg(c)
      .group_by({person_id_syn = c.person_id_syn})
      .to_array({claims = c}))
    // TODO(catalin): two issues to investigate:
    // - For some reasone the classes that I want to pickle need to be
    // defined in a different module. Why ??
    // - generators don't work here.
    .map(c => examples.claim.Assignment(
            person_id = c.person_id_syn,
            assigned = assignMember(c.person_id_syn, c.claims, 2u)));
}

demo_dir = flags.define(
  "/tmp/demodata", "demo_dir",
  "Use this directory for storing temporary data");
use_dataset = flags.define(
  true, "use_dataset",
  "Use the dataset based implementation for demo, else use arrays");

def main_function main() => {
  dir = flags.get_string(demo_dir)
  size = 0u
  if (flags.get_bool(use_dataset)) {
    engine = dataset_spark.default_local_engine("Assign Demo")
    size = analyze_dataset(engine, generate_parquet(dir)).collect()
      .map(t => print(t)).to_array().len()
  } else {
    size = analyze_array(generate_csv(dir)).to_array().len()
  }
  print("Final size: " + str(size))
}
